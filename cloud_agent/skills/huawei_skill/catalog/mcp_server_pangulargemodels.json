{
  "server": "mcp_server_pangulargemodels",
  "mode": "openapi",
  "module": "huaweicloud_services_server.mcp_server_pangulargemodels.src.mcp_server_pangulargemodels.run",
  "serviceCode": "modelarts",
  "openapiFile": "mcp-server/huaweicloud_services_server/mcp_server_pangulargemodels/src/mcp_server_pangulargemodels/config/pangulargemodels.json",
  "toolCount": 2,
  "tools": [
    {
      "name": "ExecuteTextCompletion",
      "description": "给定一个提示和一些参数,模型会根据这些信息生成一个或多个预测的补全,还可以返回每个位置上不同词语的概率。它可以用来做文本生成、自动写作、代码补全等任务。",
      "inputSchema": {
        "type": "object",
        "properties": {
          "deployment_id": {
            "example": "12928884-xxxx-xxxx-xxxx-b745b2rf",
            "description": "模型的部署ID",
            "type": "string",
            "maxLength": 255,
            "minLength": 1,
            "in": "path"
          },
          "project_id": {
            "example": "2b31ed520xxxxxxebedb6e57xxxxxxxx",
            "description": "项目ID",
            "type": "string",
            "maxLength": 255,
            "minLength": 1,
            "in": "path"
          },
          "prompt": {
            "type": "string",
            "description": "向模型输入的文本信息,最小长度:1,最大长度:模型支持的max_tokens数量乘以系数,默认系数为1.5",
            "minLength": 1
          },
          "user": {
            "type": "string",
            "description": "用于代表客户的唯一标识符,最小长度:1,最大长度64",
            "minLength": 1,
            "maxLength": 64
          },
          "stream": {
            "type": "boolean",
            "description": "流式调用的开启开关,true为开启流式调用,如果要开启流式调用,请使用流式SDK;false为关闭流式调用。默认为关闭状态(当前API Explorer不支持流式,在API Explorer调试时请使用非流式)。"
          },
          "temperature": {
            "type": "number",
            "format": "float",
            "description": "用于控制生成文本的多样性和创造力。参数的取值范围是0到1,其中0表示最低的随机性。一般来说,temperature越低,适合完成确定性的任务。temperature越高,例如0.9,适合完成创造性的任务。temperature参数可以影响语言模型输出的质量和多样性,但也不是唯一的因素。还有其他一些参数,如top_p参数也可以用来调整语言模型的行为和偏好,但不建议同时更改这两个参数。",
            "example": 1,
            "minimum": 0,
            "maximum": 1
          },
          "top_p": {
            "type": "number",
            "format": "float",
            "description": "一种替代温度采样的方法,称为nucleus sampling,其中模型考虑具有top_p 概率质量的标记的结果。因此 0.1 意味着只考虑构成前 10% 概率质量的标记。我们通常建议更改此值或温度,但不要同时更改两者。通常建议更改top_p或temperature来调整生成文本的倾向性,但不要同时更改这两个参数。",
            "minimum": 0,
            "exclusiveMinimum": true,
            "maximum": 1
          },
          "max_tokens": {
            "type": "integer",
            "format": "int32",
            "description": "用于控制聊天回复的长度和质量。一般来说,较大的max_tokens值可以生成较长和较完整的回复,但也可能增加生成无关或重复内容的风险。较小的max_tokens值可以生成较短和较简洁的回复,但也可能导致生成不完整或不连贯的内容。因此,需要根据不同的场景和需求来选择合适的max_tokens值。最小值:1,最大值:根据模型不同最大值不同。",
            "example": 16,
            "minimum": 1
          },
          "n": {
            "type": "integer",
            "format": "int32",
            "description": "表示对每个问题生成多少条答案。n参数的默认值是1,表示只生成一个答案。如果想要生成多条答案,可以设置n参数为一个大于1的整数,例如n=2。这样,API会返回一个包含2个答案的数组。流式调用时,n只能取1。最小值:1,最大值:2,默认值:1",
            "default": 1,
            "example": 1,
            "minimum": 1,
            "maximum": 2
          },
          "presence_penalty": {
            "type": "number",
            "format": "float",
            "description": "用于控制生成文本中的重复程度。正值会根据它们到目前为止在文本中的现有频率来惩罚新tokens,从而降低模型逐字重复同一行的可能性。  presence_penalty 参数可以用来提高生成文本的多样性和创造性,避免生成单调或重复的内容。最小值:-2,最大值:2",
            "example": 0,
            "minimum": -2,
            "maximum": 2
          }
        },
        "required": [
          "deployment_id",
          "project_id",
          "prompt"
        ]
      }
    },
    {
      "name": "ExecuteChatCompletion",
      "description": "基于对话问答功能,用户可以与模型进行自然而流畅的对话和交流。",
      "inputSchema": {
        "type": "object",
        "properties": {
          "deployment_id": {
            "example": "12928884-xxxx-xxxx-xxxx-b745b2rf",
            "description": "模型的部署ID",
            "type": "string",
            "maxLength": 255,
            "minLength": 1,
            "in": "path"
          },
          "project_id": {
            "example": "2b31ed520xxxxxxebedb6e57xxxxxxxx",
            "description": "项目ID",
            "type": "string",
            "maxLength": 255,
            "minLength": 1,
            "in": "path"
          },
          "messages": {
            "type": "array",
            "items": {
              "type": "object",
              "required": [
                "content"
              ],
              "properties": {
                "role": {
                  "type": "string",
                  "description": "角色",
                  "minLength": 1,
                  "maxLength": 64
                },
                "content": {
                  "type": "string",
                  "description": "问答对文本内容,最小长度:1,最大长度:模型支持的max_tokens数量乘以系数,默认系数为1.5,并且所有content的总长度不能超过该最大长度",
                  "minLength": 1
                }
              }
            },
            "description": "多轮对话问答对",
            "minItems": 1,
            "maxItems": 20
          },
          "user": {
            "type": "string",
            "description": "用于代表客户的唯一标识符,最小长度:1,最大长度64",
            "minLength": 1,
            "maxLength": 64
          },
          "stream": {
            "type": "boolean",
            "description": "流式调用的开启开关,true为开启流式调用,如果要开启流式调用,请使用流式SDK;false为关闭流式调用。默认为关闭状态(当前API Explorer不支持流式,在API Explorer调试时请使用非流式)。"
          },
          "temperature": {
            "type": "number",
            "format": "float",
            "description": "用于控制生成文本的多样性和创造力。参数的取值范围是0到1,其中0表示最低的随机性。一般来说,temperature越低,适合完成确定性的任务。temperature越高,例如0.9,适合完成创造性的任务。temperature参数可以影响语言模型输出的质量和多样性,但也不是唯一的因素。还有其他一些参数,如top_p参数也可以用来调整语言模型的行为和偏好,但不建议同时更改这两个参数。",
            "example": 1,
            "minimum": 0,
            "maximum": 1
          },
          "top_p": {
            "type": "number",
            "format": "float",
            "description": "一种替代温度采样的方法,称为nucleus sampling,其中模型考虑具有top_p 概率质量的标记的结果。因此 0.1 意味着只考虑构成前 10% 概率质量的标记。我们通常建议更改此值或温度,但不要同时更改两者。通常建议更改top_p或temperature来调整生成文本的倾向性,但不要同时更改这两个参数。",
            "minimum": 0,
            "exclusiveMinimum": true,
            "maximum": 1
          },
          "max_tokens": {
            "type": "integer",
            "format": "int32",
            "description": "用于控制聊天回复的长度和质量。一般来说,较大的max_tokens值可以生成较长和较完整的回复,但也可能增加生成无关或重复内容的风险。较小的max_tokens值可以生成较短和较简洁的回复,但也可能导致生成不完整或不连贯的内容。因此,需要根据不同的场景和需求来选择合适的max_tokens值。最小值:1,最大值:根据模型不同最大值不同。",
            "example": 16,
            "minimum": 1
          },
          "n": {
            "type": "integer",
            "format": "int32",
            "description": "表示对每个问题生成多少条答案。n参数的默认值是1,表示只生成一个答案。如果想要生成多条答案,可以设置n参数为一个大于1的整数,例如n=2。这样,API会返回一个包含2个答案的数组。流式调用时,n只能取1。最小值:1,最大值:2,默认值:1",
            "default": 1,
            "example": 1,
            "minimum": 1,
            "maximum": 2
          },
          "presence_penalty": {
            "type": "number",
            "format": "float",
            "description": "用于控制生成文本中的重复程度。正值会根据它们到目前为止在文本中的现有频率来惩罚新tokens,从而降低模型逐字重复同一行的可能性。  presence_penalty 参数可以用来提高生成文本的多样性和创造性,避免生成单调或重复的内容。最小值:-2,最大值:2",
            "example": 0,
            "minimum": -2,
            "maximum": 2
          }
        },
        "required": [
          "deployment_id",
          "messages",
          "project_id"
        ]
      }
    }
  ]
}
